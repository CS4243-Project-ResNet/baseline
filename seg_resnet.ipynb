{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tianqi/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7f044c467550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from math import floor\n",
    "import cv2\n",
    "\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/t/tianqi/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2022-11-10 Python-3.8.10 torch-1.12.1+cu102 CUDA:0 (NVIDIA TITAN RTX, 24220MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "Using cache found in /home/t/tianqi/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2022-11-10 Python-3.8.10 torch-1.12.1+cu102 CUDA:0 (NVIDIA TITAN RTX, 24220MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7012822 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "gun_model = torch.hub.load('ultralytics/yolov5', 'custom', path='/home/t/tianqi/CS4243_proj/my_utils/best1110.pt')\n",
    "gun_model.conf = 0.1\n",
    "knife_model = torch.hub.load('ultralytics/yolov5', 'custom', path='/home/t/tianqi/CS4243_proj/my_utils/knife_best.pt')\n",
    "knife_model.conf = 0.01\n",
    "\n",
    "def get_classifcation_bounding_box(file_path, model, asize):\n",
    "    results = model(file_path, size=asize)\n",
    "\n",
    "    objects = []\n",
    "    for obj in results.xyxy[0]:\n",
    "        objects.append(\n",
    "            {\n",
    "                \"class\": int(obj[5]), \n",
    "                \"xmin\": float(obj[0]),\n",
    "                \"ymin\": float(obj[1]),\n",
    "                \"xmax\": float(obj[2]),\n",
    "                \"ymax\": float(obj[3]),\n",
    "            })\n",
    "    return objects\n",
    "\n",
    "def detect_overlap(mask, xmin, xmax, ymin, ymax):\n",
    "    return np.count_nonzero(mask[ymin:ymax, xmin:xmax]) > 0\n",
    "\n",
    "def get_seg_file(img_path):\n",
    "    paths = img_path.split('/')\n",
    "    paths[0] = \"seg\"\n",
    "    seg_dir = '/'.join(paths)\n",
    "    im_read = cv2.imread(seg_dir, 0)\n",
    "    return im_read\n",
    "\n",
    "def get_seg_bin(img_path, thres=50):\n",
    "    seg = get_seg_file(img_path)\n",
    "    ret, mask = cv2.threshold(seg, thres, 1, cv2.THRESH_BINARY)\n",
    "    return mask\n",
    "\n",
    "count = 0\n",
    "def combine_mask_bounding_box(file_path, mask_thres, box_thres):\n",
    "    global count\n",
    "    gun_boxes = get_classifcation_bounding_box(file_path, gun_model, 540)\n",
    "    knife_boxes = get_classifcation_bounding_box(file_path, knife_model, 640)\n",
    "    count += len(knife_boxes)\n",
    "\n",
    "    boxes = gun_boxes + knife_boxes\n",
    "    mask = get_seg_bin(file_path, thres=mask_thres)\n",
    "    r, c = mask.shape\n",
    "\n",
    "    for box in boxes:\n",
    "        xmin, xmax, ymin, ymax = floor(box[\"xmin\"]), floor(box[\"xmax\"]), floor(box[\"ymin\"]), floor(box[\"ymax\"])\n",
    "        if detect_overlap(mask, xmin, xmax, ymin, ymax):\n",
    "            mask[max(0, ymin-box_thres):min(ymax+box_thres, r), max(0, xmin-box_thres):min(xmax+box_thres, c)] = 1\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.segs = []\n",
    "        self.labels = []\n",
    "        self.__preprocess__()\n",
    "\n",
    "\n",
    "    def __preprocess__(self):\n",
    "        subfolders = ['carrying', 'normal', 'threat']\n",
    "        for i in range(len(subfolders)):\n",
    "            # print(subfolders[i])\n",
    "            files = os.listdir(os.path.join(self.root_dir, subfolders[i]))\n",
    "            for f in files:\n",
    "                img_path = os.path.join(self.root_dir, subfolders[i], f)\n",
    "                mask = combine_mask_bounding_box(img_path, 0, 50)\n",
    "                img = cv2.imread(img_path)\n",
    "\n",
    "                seg_img = cv2.bitwise_and(img, img, mask = mask)\n",
    "\n",
    "                self.segs.append(seg_img)\n",
    "                self.labels.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = self.segs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "data_dir = 'data'\n",
    "image_datasets = {x: SegDataset(os.path.join(data_dir, x), transform=data_transforms)\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "# class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='NVIDIA TITAN RTX', major=7, minor=5, total_memory=24220MB, multi_processor_count=72)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(torch.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tianqi/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/t/tianqi/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "learning_rate = 0.0005\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n",
      "Epoch 0/39\n",
      "----------\n",
      "train Loss: 1.1365 Acc: 0.4005\n",
      "val Loss: 0.9216 Acc: 0.5223\n",
      "test Loss: 0.9081 Acc: 0.5632\n",
      "\n",
      "Epoch 1/39\n",
      "----------\n",
      "train Loss: 0.9718 Acc: 0.5068\n",
      "val Loss: 0.8467 Acc: 0.6089\n",
      "test Loss: 0.8402 Acc: 0.6053\n",
      "\n",
      "Epoch 2/39\n",
      "----------\n",
      "train Loss: 0.8109 Acc: 0.6350\n",
      "val Loss: 0.9210 Acc: 0.5932\n",
      "test Loss: 0.8855 Acc: 0.6158\n",
      "\n",
      "Epoch 3/39\n",
      "----------\n",
      "train Loss: 0.6479 Acc: 0.7232\n",
      "val Loss: 0.7912 Acc: 0.7297\n",
      "test Loss: 0.7774 Acc: 0.7211\n",
      "\n",
      "Epoch 4/39\n",
      "----------\n",
      "train Loss: 0.5697 Acc: 0.7677\n",
      "val Loss: 0.6185 Acc: 0.7585\n",
      "test Loss: 0.6117 Acc: 0.7526\n",
      "\n",
      "Epoch 5/39\n",
      "----------\n",
      "train Loss: 0.5134 Acc: 0.7851\n",
      "val Loss: 0.5993 Acc: 0.7848\n",
      "test Loss: 0.6235 Acc: 0.7684\n",
      "\n",
      "Epoch 6/39\n",
      "----------\n",
      "train Loss: 0.4649 Acc: 0.8183\n",
      "val Loss: 0.8309 Acc: 0.7113\n",
      "test Loss: 0.8018 Acc: 0.6947\n",
      "\n",
      "Epoch 7/39\n",
      "----------\n",
      "train Loss: 0.3359 Acc: 0.8786\n",
      "val Loss: 0.4508 Acc: 0.8320\n",
      "test Loss: 0.4501 Acc: 0.8211\n",
      "\n",
      "Epoch 8/39\n",
      "----------\n",
      "train Loss: 0.2592 Acc: 0.9125\n",
      "val Loss: 0.4358 Acc: 0.8399\n",
      "test Loss: 0.4346 Acc: 0.8263\n",
      "\n",
      "Epoch 9/39\n",
      "----------\n",
      "train Loss: 0.2635 Acc: 0.9042\n",
      "val Loss: 0.4082 Acc: 0.8609\n",
      "test Loss: 0.4422 Acc: 0.7947\n",
      "\n",
      "Epoch 10/39\n",
      "----------\n",
      "train Loss: 0.2220 Acc: 0.9238\n",
      "val Loss: 0.4084 Acc: 0.8451\n",
      "test Loss: 0.4134 Acc: 0.8368\n",
      "\n",
      "Epoch 11/39\n",
      "----------\n",
      "train Loss: 0.2229 Acc: 0.9246\n",
      "val Loss: 0.4380 Acc: 0.8504\n",
      "test Loss: 0.4343 Acc: 0.8263\n",
      "\n",
      "Epoch 12/39\n",
      "----------\n",
      "train Loss: 0.2014 Acc: 0.9374\n",
      "val Loss: 0.4367 Acc: 0.8478\n",
      "test Loss: 0.4635 Acc: 0.7947\n",
      "\n",
      "Epoch 13/39\n",
      "----------\n",
      "train Loss: 0.1999 Acc: 0.9314\n",
      "val Loss: 0.3809 Acc: 0.8635\n",
      "test Loss: 0.4096 Acc: 0.8316\n",
      "\n",
      "Epoch 14/39\n",
      "----------\n",
      "train Loss: 0.1991 Acc: 0.9291\n",
      "val Loss: 0.3875 Acc: 0.8688\n",
      "test Loss: 0.3675 Acc: 0.8211\n",
      "\n",
      "Epoch 15/39\n",
      "----------\n",
      "train Loss: 0.1721 Acc: 0.9389\n",
      "val Loss: 0.4312 Acc: 0.8373\n",
      "test Loss: 0.3662 Acc: 0.8632\n",
      "\n",
      "Epoch 16/39\n",
      "----------\n",
      "train Loss: 0.1836 Acc: 0.9397\n",
      "val Loss: 0.4039 Acc: 0.8530\n",
      "test Loss: 0.4097 Acc: 0.8474\n",
      "\n",
      "Epoch 17/39\n",
      "----------\n",
      "train Loss: 0.1675 Acc: 0.9457\n",
      "val Loss: 0.4399 Acc: 0.8556\n",
      "test Loss: 0.4434 Acc: 0.8053\n",
      "\n",
      "Epoch 18/39\n",
      "----------\n",
      "train Loss: 0.1702 Acc: 0.9465\n",
      "val Loss: 0.4358 Acc: 0.8478\n",
      "test Loss: 0.4292 Acc: 0.8000\n",
      "\n",
      "Epoch 19/39\n",
      "----------\n",
      "train Loss: 0.1474 Acc: 0.9548\n",
      "val Loss: 0.4291 Acc: 0.8478\n",
      "test Loss: 0.4307 Acc: 0.8105\n",
      "\n",
      "Epoch 20/39\n",
      "----------\n",
      "train Loss: 0.1680 Acc: 0.9495\n",
      "val Loss: 0.4091 Acc: 0.8478\n",
      "test Loss: 0.4398 Acc: 0.8474\n",
      "\n",
      "Epoch 21/39\n",
      "----------\n",
      "train Loss: 0.1912 Acc: 0.9404\n",
      "val Loss: 0.4112 Acc: 0.8451\n",
      "test Loss: 0.4167 Acc: 0.8316\n",
      "\n",
      "Epoch 22/39\n",
      "----------\n",
      "train Loss: 0.1583 Acc: 0.9525\n",
      "val Loss: 0.4274 Acc: 0.8556\n",
      "test Loss: 0.4137 Acc: 0.8263\n",
      "\n",
      "Epoch 23/39\n",
      "----------\n",
      "train Loss: 0.1541 Acc: 0.9487\n",
      "val Loss: 0.3857 Acc: 0.8556\n",
      "test Loss: 0.3877 Acc: 0.8368\n",
      "\n",
      "Epoch 24/39\n",
      "----------\n",
      "train Loss: 0.1835 Acc: 0.9404\n",
      "val Loss: 0.4340 Acc: 0.8425\n",
      "test Loss: 0.4366 Acc: 0.8158\n",
      "\n",
      "Epoch 25/39\n",
      "----------\n",
      "train Loss: 0.1827 Acc: 0.9397\n",
      "val Loss: 0.4005 Acc: 0.8451\n",
      "test Loss: 0.4509 Acc: 0.8105\n",
      "\n",
      "Epoch 26/39\n",
      "----------\n",
      "train Loss: 0.1792 Acc: 0.9404\n",
      "val Loss: 0.4133 Acc: 0.8373\n",
      "test Loss: 0.3880 Acc: 0.8632\n",
      "\n",
      "Epoch 27/39\n",
      "----------\n",
      "train Loss: 0.1730 Acc: 0.9434\n",
      "val Loss: 0.3953 Acc: 0.8609\n",
      "test Loss: 0.4002 Acc: 0.8421\n",
      "\n",
      "Epoch 28/39\n",
      "----------\n",
      "train Loss: 0.1700 Acc: 0.9449\n",
      "val Loss: 0.4137 Acc: 0.8583\n",
      "test Loss: 0.3964 Acc: 0.8421\n",
      "\n",
      "Epoch 29/39\n",
      "----------\n",
      "train Loss: 0.1913 Acc: 0.9284\n",
      "val Loss: 0.3945 Acc: 0.8504\n",
      "test Loss: 0.4045 Acc: 0.8263\n",
      "\n",
      "Epoch 30/39\n",
      "----------\n",
      "train Loss: 0.1855 Acc: 0.9351\n",
      "val Loss: 0.4179 Acc: 0.8504\n",
      "test Loss: 0.3786 Acc: 0.8579\n",
      "\n",
      "Epoch 31/39\n",
      "----------\n",
      "train Loss: 0.1734 Acc: 0.9419\n",
      "val Loss: 0.4066 Acc: 0.8530\n",
      "test Loss: 0.3803 Acc: 0.8368\n",
      "\n",
      "Epoch 32/39\n",
      "----------\n",
      "train Loss: 0.1900 Acc: 0.9374\n",
      "val Loss: 0.4050 Acc: 0.8451\n",
      "test Loss: 0.3953 Acc: 0.8579\n",
      "\n",
      "Epoch 33/39\n",
      "----------\n",
      "train Loss: 0.1781 Acc: 0.9419\n",
      "val Loss: 0.3925 Acc: 0.8661\n",
      "test Loss: 0.4156 Acc: 0.8421\n",
      "\n",
      "Epoch 34/39\n",
      "----------\n",
      "train Loss: 0.1784 Acc: 0.9449\n",
      "val Loss: 0.3889 Acc: 0.8556\n",
      "test Loss: 0.4300 Acc: 0.8211\n",
      "\n",
      "Epoch 35/39\n",
      "----------\n",
      "train Loss: 0.1882 Acc: 0.9404\n",
      "val Loss: 0.4157 Acc: 0.8504\n",
      "test Loss: 0.3804 Acc: 0.8316\n",
      "\n",
      "Epoch 36/39\n",
      "----------\n",
      "train Loss: 0.1887 Acc: 0.9329\n",
      "val Loss: 0.4296 Acc: 0.8504\n",
      "test Loss: 0.4352 Acc: 0.8053\n",
      "\n",
      "Epoch 37/39\n",
      "----------\n",
      "train Loss: 0.1639 Acc: 0.9563\n",
      "val Loss: 0.4072 Acc: 0.8451\n",
      "test Loss: 0.3916 Acc: 0.8579\n",
      "\n",
      "Epoch 38/39\n",
      "----------\n",
      "train Loss: 0.1720 Acc: 0.9449\n",
      "val Loss: 0.4069 Acc: 0.8530\n",
      "test Loss: 0.4053 Acc: 0.8316\n",
      "\n",
      "Epoch 39/39\n",
      "----------\n",
      "train Loss: 0.1872 Acc: 0.9419\n",
      "val Loss: 0.4242 Acc: 0.8556\n",
      "test Loss: 0.4032 Acc: 0.8263\n",
      "\n",
      "Training complete in 34m 13s\n",
      "Best val Acc: 0.868766\n"
     ]
    }
   ],
   "source": [
    "print(learning_rate)\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), \"seg_resnet50.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.86      0.81        79\n",
      "           1       0.78      0.81      0.80        43\n",
      "           2       0.88      0.74      0.80        68\n",
      "\n",
      "    accuracy                           0.81       190\n",
      "   macro avg       0.81      0.80      0.80       190\n",
      "weighted avg       0.81      0.81      0.80       190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "model_ft.eval()\n",
    "\n",
    "for inputs, labels in dataloaders[\"test\"]:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = model_ft(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    y_pred.extend(preds.data.cpu())\n",
    "    y_true.extend(labels.data.cpu())\n",
    "\n",
    "\n",
    "print(classification_report(y_true, y_pred, labels=[0,1,2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
